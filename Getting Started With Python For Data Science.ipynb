{
 "metadata": {
  "name": "Getting Started With Python For Data Science"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "\n",
      "Getting Started With Python For Data Science"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adapted from the article [Getting Started With Python For Data Science](http://example.com/https://www.kaggle.com/wiki/GettingStartedWithPythonForDataScience) by Cassius Butcher"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Who is this for and what will I learn?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This tutorial assumes some knowledge of Python and programming, but no knowledge whatsoever of data science, machine learning, or predictive modeling (or, heck, even statistics). To the extent there is a target audience, it's probably hacker types who learn best by doing.\n",
      "\n",
      "You might encounter terms you're not familiar with, but that shouldn't stop you from completing the tutorial. By the end, you won't know a heck of a lot more about data science per se, but you'll have a nice environment set up where you can easily play with lots of different data science tools and even make credible entries to Kaggle competitions. Most importantly you'll be in a great position to experiment and learn more data science.\n",
      "\n",
      "Here's what you'll learn:\n",
      "\n",
      "* How to install popular scientific and statistical computing libraries for Python\n",
      "* Use those libraries to create a benchmark predictive model and submit it to a competition.\n",
      "* Write your own evaluation function, and learn how to use cross-validation to test out ideas on your own.\n",
      "* Use randomized search for hyperparameter estimation\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Environment Setup"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First thing, we'll need a Python environment suitable for scientific and statistical computing. Assuming you already have Python installed (no? Well then get it! Python 2.7 is recommended), we'll need three packages. You should install each in the order they appear here:\n",
      "\n",
      "* numpy - (pronounced num-pie) Powerful numerical arrays. A foundational package for the two packages below.\n",
      "* scipy - (sigh-pie) Scientific, mathematical, and engineering package\n",
      "* scikit-learn - Easy to use machine learning library\n",
      " \n",
      "Note: I needed to upgrade the version of Numpy on my Ubuntu 12.04 Scikit-learn VM in order to get this tutorial to work.\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "pip install numpy --upgrade"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now check program versions below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Check program versions\n",
      "import sklearn as sk\n",
      "import numpy as np\n",
      "import IPython\n",
      "\n",
      "print sk.__version__\n",
      "print np.__version__\n",
      "print IPython.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.14.1\n",
        "1.8.1\n",
        "0.13.2\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Your First Submission"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The [Biological Response](https://www.kaggle.com/c/bioresponse) competition provides a great data set to get started with because the value to be predicted is a simple binary classifier (0 or 1) and the data is just a bunch of numbers, so feature extraction and selection aren't as important as in some other Kaggle competitions. [Download]() the training and test data sets now. Even though this competition is over, you can still make submissions and see how you compare to the world's best data scientists.\n",
      "\n",
      "In the code below, we'll use an ensemble classifier called a random forest that often performs very well as-is, without much babysitting and parameter-tweaking. Although a random forest is actually a pretty sophisticated classifier, it's a piece of cake to get up and running with one thanks to sklearn.\n",
      "\n",
      "Remember: You don't have to understand all of the underlying mathematics to use these techniques. Experimentation is a great way to start getting a feel for how this stuff works. Understanding the models is important, but it's not necessary to get started, have fun, and compete.\n",
      "\n",
      "Here's the code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from numpy import genfromtxt, savetxt\n",
      "\n",
      "\n",
      "#create the training & test sets, skipping the header row with [1:]\n",
      "dataset = genfromtxt(open('Data/train.csv','r'), delimiter=',', dtype='f8')[1:]    \n",
      "target = [x[0] for x in dataset]\n",
      "train = [x[1:] for x in dataset]\n",
      "test = genfromtxt(open('Data/test.csv','r'), delimiter=',', dtype='f8')[1:]\n",
      "\n",
      "#create and train the random forest\n",
      "#multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)\n",
      "rf = RandomForestClassifier(n_estimators=100, n_jobs=2)\n",
      "rf.fit(train, target)\n",
      "predicted_probs = [[index + 1, x[1]] for index, x in enumerate(rf.predict_proba(test))]\n",
      "\n",
      "savetxt('Data/original_submission.csv', predicted_probs, delimiter=',', fmt='%d,%f', header='MoleculeId,PredictedProbability', comments = '')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once you've run makeSubmission, you'll also have my_first_submission.csv in your data folder. Now it is time to do something very important:\n",
      "\n",
      "Submit this file to the [bio-response competition](https://www.kaggle.com/c/bioresponse/submissions/attach).\n",
      "\n",
      "The intitial submission produced a score on Kaggle of 0.41604.\n",
      "\n",
      "Did you do it? You did it! Great! You could stop here - you know how to make a successful Kaggle entry - but if you're the curious type, I'm sure you'll want to play around with other types of models, different parameters, and other shiny things. Keep reading to learn an important technique that will let you test models locally, without burning through your daily Kaggle submission limit."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evaluation and Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's say we wanted to try out sklearn's gradient boosting machine instead of a random forest. Or maybe some simple linear models. It's easy enough to import these methods from sklearn and generate submission files, but it's not so easy to compare their performance. It's not practical to upload a new submission every time we make a change to the model - we'll need a way to test things out locally, and we'll need two things in order to do that:\n",
      "\n",
      "1. An evaluation function\n",
      "2. Cross validation\n",
      "\n",
      "You'll always need some kind of evaluation function to determine how your models are performing. Ideally, these would be identical to the evaluation metric that Kaggle is using to score your entry. Competition participants often post evaluation code in the forums, and Kaggle has detailed descriptions of the metrics available on the wiki. In the case of the bio-response competition, the evaluation metric is log-loss and user Grunthus has posted a Python version of it. We won't spend too much time on this (read the forum post for more information), but go ahead and save the following into your working directory as logloss.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "def llfun(act, pred):\n",
      "    epsilon = 1e-15\n",
      "    pred = sp.maximum(epsilon, pred)\n",
      "    pred = sp.minimum(1-epsilon, pred)\n",
      "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
      "    ll = ll * -1.0/len(act)\n",
      "    return ll"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we'll need data to test our models against. When you submitted your first Kaggle competition entry earlier in this tutorial, Kaggle compared (using log-loss) your answers to the actual real world results (the \"ground truth\") associated with the test data set. Without access to those answers, how can we actually test our models locally? Cross-validation to the rescue!\n",
      "\n",
      "Cross-validation is a simple technique that basically grabs a chunk of the training data and holds it in reserve while the model is trained on the remainder of the data set. In case you haven't realized it yet, sklearn is totally awesome and is here to help. It has built in tools to generate cross validation sets. The sklearn documentation has a lot of great information on [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html).\n",
      "\n",
      "The code below creates 10 cross-validation sets (called folds), each with 10% of the training data set held in reserve, and tests our random forest model against that withheld data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import cross_validation\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "#read in  data, parse into training and target sets\n",
      "dataset = np.genfromtxt(open('Data/train.csv','r'), delimiter=',', dtype='f8')[1:]    \n",
      "target = np.array([x[0] for x in dataset])\n",
      "#print target\n",
      "train = np.array([x[1:] for x in dataset])\n",
      "#print train\n",
      "\n",
      "#Simple K-Fold cross validation. 5 folds.\n",
      "cv = cross_validation.KFold(len(train), n_folds=5, indices=False)\n",
      "\n",
      "#iterate through the training and test cross validation segments and\n",
      "#run the classifier on each one, aggregating the results into a list\n",
      "results = []\n",
      "for traincv, testcv in cv:\n",
      "    probas = rf.fit(train[traincv], target[traincv]).predict_proba(train[testcv])\n",
      "    results.append(llfun(target[testcv], [x[1] for x in probas]) )\n",
      "\n",
      "#print out the mean of the cross-validated results\n",
      "print \"Results: \" + str( np.array(results).mean() )\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that your cross-validated results might not exactly match the score Kaggle gives you on the model. This could be for a variety of (legitimate) reasons: random forests have a random component and won't yield identical results every time; the actual test set might deviate from the training set (especially when the sample size is fairly low, like in the bio-response competition); the evaluation implementation might differ slightly. Even if you are getting slightly different results, you can compare model performance locally and you should know when you have made an improvement."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Using randomized search for hyperparameter estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use randomized search and grid search for optimizing hyperparameters of a random forest. All parameters that influence the learning are searched simultaneously (except for the number of estimators, which poses a time / quality tradeoff).\n",
      "\n",
      "The randomized search and the grid search explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower.\n",
      "\n",
      "The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.\n",
      "\n",
      "Note that in practice, one would not search over this many different parameters simultaneously using grid search, but pick only the ones deemed most important.\n",
      "\n",
      "For more information see [Comparing randomized search and grid search for hyperparameter estimation](http://scikit-learn.org/stable/auto_examples/randomized_search.html#comparing-randomized-search-and-grid-search-for-hyperparameter-estimation)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from time import time\n",
      "from operator import itemgetter\n",
      "from scipy.stats import randint as sp_randint\n",
      "\n",
      "from sklearn.grid_search import RandomizedSearchCV\n",
      "\n",
      "# Utility function to report best scores\n",
      "def report(grid_scores, n_top=3):\n",
      "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
      "    for i, score in enumerate(top_scores):\n",
      "        print(\"Model with rank: {0}\".format(i + 1))\n",
      "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
      "              score.mean_validation_score,\n",
      "              np.std(score.cv_validation_scores)))\n",
      "        print(\"Parameters: {0}\".format(score.parameters))\n",
      "        print(\"\")\n",
      "\n",
      "\n",
      "# specify parameters and distributions to sample from\n",
      "param_dist = {\"max_depth\": [3, None],\n",
      "              \"max_features\": sp_randint(1, 11),\n",
      "              \"min_samples_split\": sp_randint(1, 11),\n",
      "              \"min_samples_leaf\": sp_randint(1, 11),\n",
      "              \"bootstrap\": [True, False],\n",
      "              \"criterion\": [\"gini\", \"entropy\"]}\n",
      "\n",
      "# run randomized search\n",
      "n_iter_search = 20\n",
      "random_search = RandomizedSearchCV(rf, param_distributions=param_dist,\n",
      "                                   n_iter=n_iter_search)\n",
      "\n",
      "start = time()\n",
      "random_search.fit(train, target)\n",
      "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
      "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
      "report(random_search.grid_scores_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RandomizedSearchCV took 86.54 seconds for 20 candidates parameter settings.\n",
        "Model with rank: 1\n",
        "Mean validation score: 0.783 (std: 0.011)\n",
        "Parameters: {'bootstrap': False, 'min_samples_leaf': 6, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 7, 'max_depth': None}\n",
        "\n",
        "Model with rank: 2\n",
        "Mean validation score: 0.782 (std: 0.008)\n",
        "Parameters: {'bootstrap': False, 'min_samples_leaf': 5, 'min_samples_split': 5, 'criterion': 'gini', 'max_features': 4, 'max_depth': None}\n",
        "\n",
        "Model with rank: 3\n",
        "Mean validation score: 0.781 (std: 0.008)\n",
        "Parameters: {'bootstrap': False, 'min_samples_leaf': 5, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': 5, 'max_depth': None}\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Putting it all together"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from numpy import genfromtxt, savetxt\n",
      "\n",
      "def main():\n",
      "    #create the training & test sets, skipping the header row with [1:]\n",
      "    dataset = genfromtxt(open('Data/train.csv','r'), delimiter=',', dtype='f8')[1:]    \n",
      "    target = [x[0] for x in dataset]\n",
      "    train = [x[1:] for x in dataset]\n",
      "    test = genfromtxt(open('Data/test.csv','r'), delimiter=',', dtype='f8')[1:]\n",
      "\n",
      "    #create and train the random forest  \n",
      "    rf = RandomForestClassifier(n_estimators=100,\n",
      "                                criterion='gini',\n",
      "                                max_features=4,\n",
      "                                max_depth=None,\n",
      "                                min_samples_split=1,\n",
      "                                min_samples_leaf=1,\n",
      "                                bootstrap=False,\n",
      "                                n_jobs=2)\n",
      "    rf.fit(train, target)\n",
      "    predicted_probs = [[index + 1, x[1]] for index, x in enumerate(rf.predict_proba(test))]\n",
      "\n",
      "    savetxt('Data/tuned_submission.csv', predicted_probs, delimiter=',', fmt='%d,%f', \n",
      "            header='MoleculeId,PredictedProbability', comments = '')\n",
      "\n",
      "if __name__==\"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The tuned submission produced a score on Kaggle of 0.47456, an improvement over the original score of 0.41604."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}